{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pu25TlRFeUvK",
        "outputId": "6073f269-707e-4dde-95ec-68acd3567967"
      },
      "id": "Pu25TlRFeUvK",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bf29008e",
      "metadata": {
        "id": "bf29008e"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "# useful stuff\n",
        "import numpy as np\n",
        "from scipy.special import expit\n",
        "from sklearn.preprocessing import normalize\n",
        "from nltk.tokenize import word_tokenize\n",
        "import copy\n",
        "import pickle\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a5cc8ade",
      "metadata": {
        "id": "a5cc8ade"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import ssl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1d55b59f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d55b59f",
        "outputId": "71b1f93c-7d5d-4a3d-91ee-bee8c7a4252b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ad8aa033",
      "metadata": {
        "id": "ad8aa033"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "\n",
        "def remove_punctuation(corpus):\n",
        "    punctuationfree = []\n",
        "    punctuationfree.extend(i for i in corpus if i not in string.punctuation)\n",
        "    return punctuationfree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "162795b1",
      "metadata": {
        "id": "162795b1"
      },
      "outputs": [],
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    output = [i for i in text if i not in stopwords]\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "40a5a98a",
      "metadata": {
        "id": "40a5a98a"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# defining the object for Lemmatization\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "# defining the function for lemmatization\n",
        "def lemmatizer(text):\n",
        "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
        "    return lemm_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "348a35b3",
      "metadata": {
        "id": "348a35b3"
      },
      "outputs": [],
      "source": [
        "def text2sentences(path):\n",
        "    # feel free to make a better tokenization/pre-processing\n",
        "    sentences = []\n",
        "    with open(path, encoding='utf-8') as f: # 加了encoding='utf-8'，否则读txt时有bug\n",
        "        for l in f:\n",
        "            sentences.append(l.lower().split())\n",
        "\n",
        "    # remove punctuations\n",
        "    sentences = [remove_punctuation(sent) for sent in sentences]\n",
        "\n",
        "    # Remove numbers\n",
        "    sentences = [list(filter(lambda x: x.isalpha(), sent)) for sent in sentences]\n",
        "\n",
        "    # Lemmatization\n",
        "    sentences = [lemmatizer(sent) for sent in sentences]\n",
        "\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ff7378c5",
      "metadata": {
        "id": "ff7378c5"
      },
      "outputs": [],
      "source": [
        "sentences = text2sentences(\"/content/drive/MyDrive/NLP-skipGram/train.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1dd507fe",
      "metadata": {
        "id": "1dd507fe"
      },
      "outputs": [],
      "source": [
        "def loadPairs(path):\n",
        "    data = pd.read_csv(path, delimiter='\\t')\n",
        "    pairs = zip(data['word1'], data['word2'], data['similarity'])\n",
        "    return pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "cb4703fa",
      "metadata": {
        "id": "cb4703fa"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + math.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "69782889",
      "metadata": {
        "id": "69782889"
      },
      "outputs": [],
      "source": [
        "def cosine_distance(vec1, vec2):\n",
        "    assert vec1.shape == vec2.shape\n",
        "    return np.dot(vec1, vec2) / (np.sqrt(np.dot(vec1, vec1)) * np.sqrt(np.dot(vec2, vec2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a003d0ce",
      "metadata": {
        "id": "a003d0ce"
      },
      "outputs": [],
      "source": [
        "class mySkipGram:\n",
        "    def __init__(self, sentences, nEmbed=100, negativeRate=5, winSize=5, minCount=5, learning=0.01):\n",
        "\n",
        "        # Storing hyper parameters as class variables\n",
        "        self.nEmbed = nEmbed\n",
        "        self.negativeRate = negativeRate\n",
        "        self.winSize = winSize\n",
        "        self.minCount = minCount\n",
        "        self.lr = learning\n",
        "\n",
        "        self.vocab_dict = self.vocab_dict_generator(sentences)\n",
        "        # word to ID mapping\n",
        "        self.w2id = {key: values['word_index'] for key, values in self.vocab_dict.items()}\n",
        "        self.trainset = sentences\n",
        "        self.vocab = list(self.vocab_dict.keys())  # list of valid words\n",
        "        self.word_freq = {values['word_index']: values['word_freq'] for key, values in self.vocab_dict.items()}\n",
        "\n",
        "        # initialize input_embedding matrix and output_weight matrix\n",
        "\n",
        "        # with a different initialization\n",
        "        self.input_embedding = np.random.uniform(low=-0.5 / (self.nEmbed ** (3 / 4)),\n",
        "                                                 high=0.5 / (self.nEmbed ** (3 / 4)),\n",
        "                                                 size=(len(self.vocab_dict), self.nEmbed))\n",
        "\n",
        "        # plain choice, all zeros\n",
        "        # self.input_embedding = np.zeros([len(self.vocab_dict), self.nEmbed])\n",
        "        self.output_weights = np.zeros([len(self.vocab_dict), self.nEmbed])\n",
        "        self.G0 = np.zeros_like(self.input_embedding)\n",
        "        self.G1 = np.zeros_like(self.output_weights)\n",
        "\n",
        "        # create variables to keep track of model performance\n",
        "        self.trainWords = 0\n",
        "        self.accLoss = 0\n",
        "        self.loss = []\n",
        "\n",
        "        # create an 'unknown_vector' to represent word in test data that doesn't exist in our train set\n",
        "        self.unknown_vector = np.random.normal(0, 1, (self.nEmbed,))\n",
        "\n",
        "    def vocab_dict_generator(self, sentences):\n",
        "        \"\"\"generate a dictionary to store information for each single word\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        sentences: a list of sentences\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        vocab: a dictionary\n",
        "        where key is unique word from corpus and value is a dictionary {'word_count': how many times a word has appeared in the entire train set,\n",
        "        'word_freq': the frequency of a word, 'word_index': assigning an id to each word}\n",
        "        \"\"\"\n",
        "\n",
        "        vocab = defaultdict(dict)\n",
        "        vocab_words = ['int']\n",
        "        vocab['int']['word_count'] = 0\n",
        "        vocab_size = 0\n",
        "        for sent_tokens in sentences:\n",
        "            vocab_size += len(sent_tokens)\n",
        "            for word in sent_tokens:\n",
        "                if word not in vocab:\n",
        "                    vocab[word]['word_count'] = 1\n",
        "                    vocab_words.append(word)\n",
        "                else:\n",
        "                    vocab[word]['word_count'] += 1\n",
        "\n",
        "        # remove words appearing fewer than min_count times\n",
        "        low_freq_words = []\n",
        "        for word in vocab:\n",
        "            if vocab[word]['word_count'] < self.minCount:\n",
        "                low_freq_words.append(word)\n",
        "        for word in low_freq_words:\n",
        "            vocab_size -= vocab[word]['word_count']\n",
        "            del vocab[word]\n",
        "            vocab_words.remove(word)\n",
        "        sorted_vocab = []\n",
        "        for word in vocab:\n",
        "            sorted_vocab.append((word, vocab[word]['word_count']))\n",
        "        sorted_vocab.sort(key=lambda tup: tup[1], reverse=True)\n",
        "        for idx, word in enumerate(sorted_vocab):\n",
        "            vocab[word[0]]['word_freq'] = vocab[word[0]]['word_count'] / vocab_size\n",
        "            vocab[word[0]]['word_index'] = idx\n",
        "        return vocab\n",
        "\n",
        "    def negative_sampling(self, word_tuple, alpha=0.75):\n",
        "        \"\"\"samples negative words, ommitting word_tuple, Words that actually appear within the context window of the center word\n",
        "            and generate ids of words that are randomly drawn from a noise distribution\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        word_tuple : tuple of {wIdx, ctxtId}\n",
        "        wIdx is the index of center word, ctxtId is the index of context word\n",
        "\n",
        "        alpha: a hyper-parameter that can be empircially tuned\n",
        "        in the noise distribution — normalized frequency distribution of words raised to the power of α.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        negativeIds: a dictionary with key as word, probability of being chosen as value\n",
        "        representing words that doesn't appear within the context window of the centre word but exist in the corpus\n",
        "\n",
        "        \"\"\"\n",
        "        word_freq_copy = copy.deepcopy(self.word_freq)\n",
        "        # remove positive sample\n",
        "        for id in word_tuple:\n",
        "            word_freq_copy.pop(id)\n",
        "\n",
        "        # generate noise distribution\n",
        "        noise_dist = {key: val ** alpha for key, val in word_freq_copy.items()}\n",
        "        Z = sum(noise_dist.values())\n",
        "        noise_dist_normalized = {key: val / Z for key, val in noise_dist.items()}\n",
        "\n",
        "        negativeIds = np.random.choice(list(noise_dist_normalized.keys()), size=self.negativeRate,\n",
        "                                       p=list(noise_dist_normalized.values()))\n",
        "\n",
        "        return negativeIds\n",
        "\n",
        "    def loss_function(self, wordId, contextId, negativeIds):\n",
        "        \"\"\" Returns the loss for the given word, its context and the negative samples\"\"\"\n",
        "\n",
        "        l_sum = np.log(sigmoid(np.dot(self.input_embedding[wordId, :], self.output_weights[contextId, :])))\n",
        "\n",
        "        for negativeId in negativeIds:\n",
        "            l_sum *= np.log(sigmoid(-np.dot(self.input_embedding[negativeId, :], self.output_weights[contextId, :])))\n",
        "\n",
        "        return l_sum\n",
        "\n",
        "    def train(self):\n",
        "        for counter, sentence in enumerate(tqdm(self.trainset)):\n",
        "            sentence = list(filter(lambda word: word in self.vocab, sentence))\n",
        "\n",
        "            for wpos, word in enumerate(sentence):\n",
        "                wIdx = self.w2id[word]\n",
        "                # dynamic window size, the winSize denotes the maximal window size. For each word in the corpus, a window size k' is randomly sampled uniformly from 1,,,,winSize\n",
        "                winsize = np.random.randint(self.winSize) + 1\n",
        "                start = max(0, wpos - winsize)\n",
        "                end = min(wpos + winsize + 1, len(sentence))\n",
        "\n",
        "                for context_word in sentence[start:end]:\n",
        "                    ctxtId = self.w2id[context_word]\n",
        "                    if ctxtId == wIdx: continue\n",
        "                    negativeIds = self.negative_sampling({wIdx, ctxtId})\n",
        "                    self.trainWord(wIdx, ctxtId, negativeIds)\n",
        "\n",
        "                    # keep record of loss during training\n",
        "                    self.accLoss += self.loss_function(wIdx, ctxtId, negativeIds)\n",
        "                    self.trainWords += 1\n",
        "\n",
        "            if (counter+1) % 100 == 0:\n",
        "                # print(' > training %d of %d' % (counter, len(self.trainset)))\n",
        "                self.loss.append(self.accLoss / self.trainWords)\n",
        "                self.trainWords = 0\n",
        "                self.accLoss = 0\n",
        "                # print(self.loss[-1])\n",
        "\n",
        "    # Back propagation\n",
        "\n",
        "    def trainWord(self, wordId, contextId, negativeIds):\n",
        "\n",
        "        # positive_loss = np.log(1 / (1 + np.exp(-np.dot(self.W[wIdx], self.C[ctxId]))))\n",
        "        # negative_loss = 0\n",
        "        # for i in negativeIds:\n",
        "        #     negative_loss += expit(-np.dot(self.W[wIdx], self.C[i]))  # remove - sign\n",
        "        # self.accLoss -= (negative_loss + positive_loss)\n",
        "\n",
        "        neg_sample = [(wordId, 1)]\n",
        "        wv_h = self.input_embedding[contextId]\n",
        "        # For each positive word-context pair (w,cpos),\n",
        "        # K new negative samples are randomly drawn from a noise distribution.\n",
        "\n",
        "        for neg_word in negativeIds:\n",
        "            neg_sample.append((neg_word, 0))\n",
        "\n",
        "        # Adagrad\n",
        "        dh = np.zeros(self.nEmbed)\n",
        "\n",
        "        for neg_w in neg_sample:\n",
        "            target, label = neg_w[0], neg_w[1]\n",
        "\n",
        "            wv_j = self.output_weights[target]\n",
        "            dwjh = sigmoid(np.dot(wv_h, wv_j)) - label\n",
        "            dwj = dwjh * wv_h\n",
        "            self.G1[target] += np.power(dwj, 2)\n",
        "            dwj /= (np.sqrt(self.G1[target]) + 1e-6)  # to avoid 0 in denominator\n",
        "            assert dwj.shape == wv_j.shape\n",
        "            dh += dwjh * wv_j\n",
        "            # Update the output weight matrix\n",
        "            self.output_weights[target] -= self.lr * dwj\n",
        "\n",
        "        # Update the input embedding matrix\n",
        "        self.G0[contextId] += np.power(dh, 2)\n",
        "        dh /= np.sqrt(self.G0[contextId]) + 1e-6\n",
        "        assert dh.shape == wv_h.shape\n",
        "        self.input_embedding[contextId] -= self.lr * dh\n",
        "\n",
        "    def save(self, path):\n",
        "        pickle.dump(self, open(path, 'wb'))\n",
        "        # with open(path, 'wb') as f:\n",
        "        #\n",
        "        #     np.save(f, self.input_embedding, allow_pickle=False, fix_imports=True)\n",
        "\n",
        "    def similarity(self, word1, word2):\n",
        "        \"\"\"\n",
        "        computes similiarity between the two words measured by consine distance. unknown words are mapped to one common vector\n",
        "        :param word1:\n",
        "        :param word2:\n",
        "        :return: a float \\in [0,1] indicating the similarity (the higher the more similar)\n",
        "        \"\"\"\n",
        "\n",
        "        if word1 == word2:\n",
        "            print('same word')\n",
        "            return 1\n",
        "        else:\n",
        "            vec1 = self.input_embedding[self.w2id[word1]] if word1 in self.vocab else self.unknown_vector\n",
        "            vec2 = self.input_embedding[self.w2id[word2]] if word2 in self.vocab else self.unknown_vector\n",
        "            return np.dot(vec1, vec2) / (np.sqrt(np.dot(vec1, vec1)) * np.sqrt(np.dot(vec2, vec2)))\n",
        "\n",
        "    @staticmethod\n",
        "    def load(path):\n",
        "        return pickle.load(open(path, 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8b97780e",
      "metadata": {
        "id": "8b97780e"
      },
      "outputs": [],
      "source": [
        "# sg = mySkipGram(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d12a44c3",
      "metadata": {
        "id": "d12a44c3"
      },
      "outputs": [],
      "source": [
        "# sg.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "94fb22ba",
      "metadata": {
        "id": "94fb22ba"
      },
      "outputs": [],
      "source": [
        "# sg.save('/content/drive/MyDrive/NLP-skipGram/saved_model/model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "80efbaa7",
      "metadata": {
        "id": "80efbaa7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "616fb2b7",
      "metadata": {
        "id": "616fb2b7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "67d508e5",
      "metadata": {
        "id": "67d508e5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d6b3efda",
      "metadata": {
        "id": "d6b3efda"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c147dd8f",
      "metadata": {
        "id": "c147dd8f",
        "outputId": "47ec999d-4cde-4ce1-8e7e-96ed204f484d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.06538296957193264]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "sg.loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "6b333266",
      "metadata": {
        "id": "6b333266"
      },
      "outputs": [],
      "source": [
        "# nEmbed=100, negativeRate=5, winSize=5, minCount=5, learning=0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "8b177e83",
      "metadata": {
        "id": "8b177e83"
      },
      "outputs": [],
      "source": [
        "nEmbed_range = np.arange(50,200,50)\n",
        "negativeRate_range = np.arange(5,20,5)\n",
        "winSize_range = np.arange(3,12,3)\n",
        "minCount_range = np.arange(0,9,3)\n",
        "learning_ragne = (0.0001,0.001,0.01,0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e99ea88e",
      "metadata": {
        "id": "e99ea88e",
        "outputId": "40946a2f-b2a5-455b-a92d-c8d99b63efe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "324"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "len(nEmbed_range)*len(negativeRate_range)*len(winSize_range)*len(minCount_range)*len(learning_ragne)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "group_dict = {}\n",
        "loss_values = {}"
      ],
      "metadata": {
        "id": "4drKoFpGBuEM"
      },
      "id": "4drKoFpGBuEM",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random grid search\n",
        "best_estimator = [0,0,0,0,0]\n",
        "min_loss = -0.14673792546101652 #float('inf')\n",
        "\n",
        "for i in range(100):\n",
        "  nEmbed = np.random.choice(nEmbed_range)\n",
        "  negativeRate = np.random.choice(negativeRate_range)\n",
        "  winSize = np.random.choice(winSize_range)\n",
        "  minCount = np.random.choice(minCount_range)\n",
        "  learning = np.random.choice(learning_ragne)\n",
        "  if [nEmbed,negativeRate,winSize,minCount,learning] not in group_dict.values():\n",
        "    group_dict[i] = [nEmbed,negativeRate,winSize,minCount,learning]\n",
        "    sg = mySkipGram(sentences,nEmbed, negativeRate, winSize, minCount, learning)\n",
        "    sg.train()\n",
        "    loss = sg.loss[-1]\n",
        "    if loss < min_loss:\n",
        "        min_loss = loss\n",
        "        best_estimator = [nEmbed, negativeRate, winSize, minCount, learning]\n",
        "        print(\"up to now, best estimator:\", best_estimator)\n",
        "        print(\"with loss\", min_loss)\n",
        "    loss_values[i] = loss\n",
        "  else:\n",
        "    continue\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GQI2aE-2wxX",
        "outputId": "0b60ac55-4245-44fb-81d5-ebb9ba53fdcd"
      },
      "id": "6GQI2aE-2wxX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [14:26<00:00,  1.15it/s]\n",
            "100%|██████████| 1000/1000 [02:20<00:00,  7.13it/s]\n",
            "100%|██████████| 1000/1000 [33:22<00:00,  2.00s/it]\n",
            "100%|██████████| 1000/1000 [06:34<00:00,  2.54it/s]\n",
            "100%|██████████| 1000/1000 [03:00<00:00,  5.55it/s]\n",
            "100%|██████████| 1000/1000 [02:43<00:00,  6.11it/s]\n",
            "100%|██████████| 1000/1000 [05:01<00:00,  3.32it/s]\n",
            "100%|██████████| 1000/1000 [04:24<00:00,  3.78it/s]\n",
            "100%|██████████| 1000/1000 [04:55<00:00,  3.38it/s]\n",
            "100%|██████████| 1000/1000 [01:21<00:00, 12.23it/s]\n",
            " 18%|█▊        | 176/1000 [05:50<33:09,  2.41s/it]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Hyper Parameter.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}